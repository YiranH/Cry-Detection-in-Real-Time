# Cry Detection
A course project for CSCE 636, Neural Networks

For every part , I will create a new branch named part$i. Please switch to the corresponding branch to see new codes, data, and results. All instructions will be put in the README file in the master branch. Please follow the instructions in the corresponding part.

PART5

1. My codes on Github: https://github.com/YiranH/Cry-Detection-in-Real-Time.git

2. My codes and data on Google Colab:

   https://drive.google.com/open?id=17LOBLJLDFgHtDvpQV39D5OBpooPrtlvv

   Demo code on Google Colab (Used in tutorial video):

   https://drive.google.com/open?id=1Nai8_Fx_Mby7pFloepShMvOGqLrP1rj-

   In Part 5, the demo code starts from the json files generated by your video, and the csv file of your label.

   Training code (To make things faster, I used 2 colab files to run different models):

   https://drive.google.com/open?id=1X_WP3rmnyhGNy0Ch56awespL_Xss_plY

   https://drive.google.com/open?id=1gqt7SnQ5LmoWmeneKyhaSd5D-pD4z6QA

3. My sample videos and tutorial video on YouTube (the first 4 videos are new for Part 5, video 5 is shared by Part 4 and Part 5):

   (https://www.youtube.com/playlist?list=PLzFb8wKb-EHHnxpzFXXUx3iGnDLuOGZDn)



In Part 4, I used a self-made dataset of 165 video clips (6781 frames) downloaded from YouTube(https://www.youtube.com/user/watchcut).

In Part 5, I made a larger dataset by collecting the crying selfie videos on Youtube and some news videos. I collected about 50 videos and picked 20 from them where people cried dramatically. In Part 4, I collected many video clips about people crying and people not crying, and there are no data about how they turn to cry or how they managed to hold back their tears. Different from the dataset I created in Part 4, I used longer videos in Part 5, so that there are data about people from cry to not cry, vice versa. I think is an huge improvement to my dataset.

For the dataset, I take every 10 frames as a timestep, which is to say, I use these 10 frames to predict the facial expression in the 10th frame. I tried to use 30 and 50 frames as a timestep but results show that 10 is better than 30 and 50.

To sum up, a total of 97068 frames are transformed into a 3-dimensional array of size (97068, 10, 156), in which 97068 is the number of samples, 10 is the timestep, and 156 is the number of features. As for the features, I used the 70 facial landmarks and the first 8 body landmarks which are nose, neck, right shoulder, right elbow, right wrist, left shoulder, left elbow, left wrist, because cry is related more to the upper part of body, and the most common reaction is to bury face in hands or wipe your eye. I also removed the confidence scores generated by OpenPose. In total I kept 78 kinds of landmarks and 156 features.
Labeling these videos are not easy, because sometimes people are not crying with their mouth open, face wrinkled, instead they cry silently, staring at the air, or mumbling. So, it is very difficult to tell whether they are crying or not. If you listen to the audio, you know they are crying, but if you just look at the picture, it’s hard to tell. I am confused with this. To make things easy, in Part 5 I only labeled positive for the frames in which I can clearly tell they are crying without the help of audio. In the future, I plan to add more label such as ‘sob’ or ‘weep’.

Since this part took me too much time, and the GPU on Google Colab platform is not very fast, there is very little time left to tune my model. In the next part, I plan to use more complex models.